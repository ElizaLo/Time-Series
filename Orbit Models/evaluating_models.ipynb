{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Orbit (**O**bject-**OR**iented **B**ayes**I**an **T**ime Series)\n",
    "\n",
    "- [Orbit: A Python Package for Bayesian Forecasting](https://github.com/uber/orbit)\n",
    "- [Orbitâ€™s Documentation](https://uber.github.io/orbit/)\n",
    "- [Quick Start](https://uber.github.io/orbit/tutorials/quick_start.html)\n",
    "- [Orbit: Probabilistic Forecast with Exponential Smoothing](https://arxiv.org/abs/2004.08492) Paper\n",
    "\n",
    "\n",
    "### Implemented Models\n",
    "\n",
    "- ETS (which stands for Error, Trend, and Seasonality) Model\n",
    "- Methods of Estimations\n",
    "    - Maximum a Posteriori (MAP)\n",
    "    - Full Bayesian Estimation\n",
    "    - Aggregated Posteriors\n",
    "- Damped Local Trend (DLT)\n",
    "    - Global Trend Configurations:\n",
    "        - Linear Global Trend\n",
    "        - Log-Linear Global Trend\n",
    "        - Flat Global Trend\n",
    "        - Logistic Global Trend\n",
    "    - Damped Local Trend Full Bayesian Estimation (DLTFull)\n",
    "- Local Global Trend (LGT)\n",
    "    - Local Global Trend Maximum a Posteriori (LGTMAP)\n",
    "    - Local Global Trend for full Bayesian prediction (LGTFull)\n",
    "    - Local Global Trend for aggregated posterior prediction (LGTAggregated)\n",
    "- Using Pyro for Estimation\n",
    "    - MAP Fit and Predict\n",
    "    - VI Fit and Predict\n",
    "- Kernel-based Time-varying Regression (KTR)\n",
    "    - Kernel-based Time-varying Regression Lite (KTRLite)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install orbit-ml --no-input"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import awswrangler as wr\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import urllib3\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import orbit\n",
    "from orbit import *\n",
    "from orbit.models.dlt import ETSFull, ETSMAP, ETSAggregated, DLTMAP, DLTFull, DLTMAP, DLTAggregated\n",
    "from orbit.models.lgt import LGTMAP, LGTAggregated, LGTFull\n",
    "from orbit.models.ktrlite import KTRLiteMAP\n",
    "\n",
    "from orbit.estimators.pyro_estimator import PyroEstimatorVI, PyroEstimatorMAP"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Uploading data\n",
    "\n",
    "- uploading data for **models**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "role = get_execution_role()\n",
    "bucket='...'\n",
    "data_key = '...csv' \n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.DataFrame(pd.read_csv(data_location))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df.rename({'Unnamed: 0': 'Date'}, axis = 1)\n",
    "df.index = df['Date']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#df = df.drop(['curve'], axis = 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Orbit Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ETS (which stands for Error, Trend, and Seasonality)\n",
    "\n",
    "# Methods of Estimations\n",
    "\n",
    "# Maximum a Posteriori (MAP)\n",
    "\n",
    "# The advantage of MAP estimation is a faster computational speed.\n",
    "\n",
    "def ETSMAP_model(date_col, response_col, train_df, test_df):\n",
    "    ets = ETSMAP(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "    )\n",
    "    \n",
    "    ets.fit(df=train_df)\n",
    "    predicted_df_MAP = ets.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_MAP['prediction']\n",
    "\n",
    "# Full Bayesian Estimation\n",
    "\n",
    "\n",
    "def ETSFull_model(date_col, response_col, train_df, test_df):\n",
    "    ets = ETSFull(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "        num_warmup=400,\n",
    "        num_sample=400,\n",
    "    )\n",
    "    \n",
    "    ets.fit(df=train_df)\n",
    "    predicted_df_ETSFull = ets.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_ETSFull['prediction']\n",
    "\n",
    "# Aggregated Posteriors\n",
    "\n",
    "def ETSAggregated_model(date_col, response_col, train_df, test_df):\n",
    "    ets = ETSAggregated(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "    )\n",
    "    ets.fit(df=train_df)\n",
    "    predicted_df_ETSAggregated = ets.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_ETSAggregated['prediction']\n",
    "\n",
    "\n",
    "# Damped Local Trend (DLT)\n",
    "\n",
    "# Global Trend Configurations\n",
    "\n",
    "# Linear Global Trend\n",
    "\n",
    "# linear global trend\n",
    "def DLTMAP_lin(date_col, response_col, train_df, test_df):\n",
    "    dlt = DLTMAP(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "    )\n",
    "\n",
    "    dlt.fit(train_df)\n",
    "    predicted_df_DLTMAP_lin = dlt.predict(test_df)\n",
    "    \n",
    "    return predicted_df_DLTMAP_lin['prediction']\n",
    "\n",
    "\n",
    "# log-linear global trend\n",
    "def DLTMAP_log_lin(date_col, response_col, train_df, test_df):\n",
    "    dlt = DLTMAP(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "        global_trend_option='loglinear'\n",
    "    )\n",
    "\n",
    "    dlt.fit(train_df)\n",
    "    predicted_df_DLTMAP_log_lin = dlt.predict(test_df)\n",
    "    \n",
    "    return predicted_df_DLTMAP_log_lin['prediction']\n",
    "\n",
    "\n",
    "# log-linear global trend\n",
    "def DLTMAP_flat(date_col, response_col, train_df, test_df):\n",
    "    dlt = DLTMAP(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "        global_trend_option='flat'\n",
    "    )\n",
    "\n",
    "    dlt.fit(train_df)\n",
    "    predicted_df_DLTMAP_flat = dlt.predict(test_df)\n",
    "    \n",
    "    return predicted_df_DLTMAP_flat['prediction']\n",
    "\n",
    "\n",
    "# logistic global trend\n",
    "def DLTMAP_logistic(date_col, response_col, train_df, test_df):\n",
    "    dlt = DLTMAP(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "        global_trend_option='logistic'\n",
    "    )\n",
    "\n",
    "    dlt.fit(train_df)\n",
    "    predicted_df_DLTMAP_logistic = dlt.predict(test_df)\n",
    "    \n",
    "    return predicted_df_DLTMAP_logistic['prediction']\n",
    "\n",
    "\n",
    "# Damped Local Trend Full Bayesian Estimation (DLTFull)\n",
    "\n",
    "def DLTFull_model(date_col, response_col, train_df, test_df):\n",
    "    dlt = DLTFull(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        num_warmup=400,\n",
    "        num_sample=400,\n",
    "        seasonality=52,\n",
    "        seed=8888\n",
    "    )\n",
    "    \n",
    "    dlt.fit(df=train_df)\n",
    "    predicted_df_DLTFull = dlt.predict(df=test_df)\n",
    "\n",
    "    return predicted_df_DLTFull['prediction']\n",
    "\n",
    "\n",
    "# Damped Local Trend Full (DLTAggregated)\n",
    "\n",
    "def DLTAggregated_model(date_col, response_col, train_df, test_df):\n",
    "    ets = DLTAggregated(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "    )\n",
    "    \n",
    "    ets.fit(df=train_df)\n",
    "    predicted_df_DLTAggregated = ets.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_DLTAggregated['prediction']\n",
    "\n",
    "\n",
    "# Local Global Trend (LGT) Model\n",
    "\n",
    "# Local Global Trend Maximum a Posteriori (LGTMAP)\n",
    "\n",
    "def LGTMAP_model(date_col, response_col, train_df, test_df):\n",
    "    lgt = LGTMAP(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "    )\n",
    "\n",
    "    lgt.fit(df=train_df)\n",
    "    predicted_df_LGTMAP = lgt.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_LGTMAP['prediction']\n",
    "\n",
    "# LGTFull\n",
    "\n",
    "def LGTFull_model(date_col, response_col, train_df, test_df):\n",
    "    lgt = LGTFull(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "    )\n",
    "\n",
    "    lgt.fit(df=train_df)\n",
    "    predicted_df_LGTFull = lgt.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_LGTFull['prediction']\n",
    "\n",
    "# LGTAggregated\n",
    "\n",
    "def LGTAggregated_model(date_col, response_col, train_df, test_df):\n",
    "    lgt = LGTAggregated(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "    )\n",
    "\n",
    "    lgt.fit(df=train_df)\n",
    "    predicted_df_LGTAggregated = lgt.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_LGTAggregated['prediction']\n",
    "\n",
    "# Using Pyro for Estimation\n",
    "\n",
    "# MAP Fit and Predict\n",
    "\n",
    "def LGTMAP_PyroEstimatorMAP(date_col, response_col, train_df, test_df):\n",
    "    lgt_map = LGTMAP(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "        estimator_type=PyroEstimatorMAP,\n",
    "    )\n",
    "\n",
    "    lgt_map.fit(df=train_df)\n",
    "    predicted_df_LGTMAP_pyro = lgt_map.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_LGTMAP_pyro['prediction']\n",
    "\n",
    "# VI Fit and Predict\n",
    "\n",
    "def LGTFull_pyro(date_col, response_col, train_df, test_df):\n",
    "    lgt_vi = LGTFull(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "        num_steps=101,\n",
    "        num_sample=100,\n",
    "        learning_rate=0.1,\n",
    "        n_bootstrap_draws=-1,\n",
    "        estimator_type=PyroEstimatorVI,\n",
    "    )\n",
    "\n",
    "    lgt_vi.fit(df=train_df)\n",
    "\n",
    "    predicted_df_LGTFull_pyro = lgt_vi.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_LGTFull_pyro['prediction']\n",
    "\n",
    "\n",
    "# Kernel-based Time-varying Regression (KTR)\n",
    "\n",
    "# KTRLite\n",
    "\n",
    "def ktrlite_MAP(date_col, response_col, train_df, test_df):\n",
    "    ktrlite = KTRLiteMAP(\n",
    "        response_col=response_col,\n",
    "        #response_col=np.log(df[response_col]),\n",
    "        date_col=date_col,\n",
    "        level_knot_scale=.1,\n",
    "        span_level=.05,\n",
    "    )\n",
    "    \n",
    "    ktrlite.fit(train_df)\n",
    "    \n",
    "    predicted_df_ktrlite_MAP = ktrlite.predict(df=test_df, decompose=True)\n",
    "    \n",
    "    return predicted_df_ktrlite_MAP['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Root-Mean-Square Deviation (RMSD) or Root-Mean-Square Error (RMSE)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def rmse(actual, pred): \n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    return np.sqrt(np.square(np.subtract(actual,pred)).mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluating_models(index, column):\n",
    "    \n",
    "    '''\n",
    "    Parameters:\n",
    "        index: column index\n",
    "        column: column name\n",
    "    \n",
    "    Returns:\n",
    "        models_df: new dataframe with \n",
    "    '''\n",
    "\n",
    "    cocompared_values = compared_function(column) # the values with which the model values are compared\n",
    "\n",
    "    tmp_df['Date'] = pd.to_datetime(df['Date'].astype(str))\n",
    "    tmp_df['Penetration'] = df[column].astype(float)\n",
    "    \n",
    "    date_col = 'Date'\n",
    "    response_col = 'Penetration'\n",
    "    \n",
    "    # Forecasting for N years ahead\n",
    "    \n",
    "    test_size = 4 # 5 - without 2021, 4 - with 2021\n",
    "    train_df = tmp_df[:-test_size]\n",
    "    test_df = tmp_df[-test_size:]\n",
    "\n",
    "    # Decompose Prediction\n",
    "\n",
    "    #train_df = tmp_df[tmp_df['Date'] < '2022-01-01']\n",
    "    #test_df = tmp_df[tmp_df['Date'] <= '2025-01-01']\n",
    "    \n",
    "    models_df.at[index ,'Item Name'] = column\n",
    "\n",
    "    \n",
    "    # Making predictions with each model\n",
    "    try:\n",
    "        models_df.at[index , 'ETSMAP'] = rmse(\n",
    "            cocompared_values, (ETSMAP_model(date_col, response_col, train_df, test_df))).astype(float)\n",
    "    except:\n",
    "        models_df.at[index , 'ETSMAP'] = 100\n",
    "    try:    \n",
    "        models_df.at[index , 'ETSFull'] = rmse(\n",
    "            cocompared_values, (ETSFull_model(date_col, response_col, train_df, test_df))).astype(float)\n",
    "    except:\n",
    "        models_df.at[index , 'ETSFull'] = 100\n",
    "    try:\n",
    "        models_df.at[index , 'ETSAggregated'] = rmse(\n",
    "            cocompared_values, (ETSAggregated_model(date_col, response_col, train_df, test_df))).astype(float)\n",
    "    except:\n",
    "        models_df.at[index , 'ETSAggregated'] = 100\n",
    "\n",
    "    \n",
    "    try:\n",
    "        models_df.at[index , 'DLTMAP_lin'] = rmse(\n",
    "            cocompared_values, (DLTMAP_lin(date_col, response_col, train_df, test_df))).astype(float)\n",
    "    except:\n",
    "        models_df.at[index , 'DLTMAP_lin'] = 100\n",
    "    try:\n",
    "        models_df.at[index , 'DLTMAP_log_lin'] = rmse(\n",
    "            cocompared_values, (DLTMAP_log_lin(date_col, response_col, train_df, test_df))).astype(float)\n",
    "    except:\n",
    "        models_df.at[index , 'DLTMAP_log_lin'] = 100\n",
    "    try:\n",
    "        models_df.at[index , 'DLTMAP_flat'] = rmse(\n",
    "            cocompared_values, (DLTMAP_flat(date_col, response_col, train_df, test_df))).astype(float)\n",
    "    except:\n",
    "        models_df.at[index , 'DLTMAP_flat'] = 100\n",
    "    try:\n",
    "        models_df.at[index , 'DLTMAP_logistic'] = rmse(\n",
    "            cocompared_values, (DLTMAP_logistic(date_col, response_col, train_df, test_df))).astype(float)\n",
    "    except:\n",
    "        models_df.at[index , 'DLTMAP_logistic'] = 100\n",
    "    try:    \n",
    "        models_df.at[index , 'DLTFull'] = rmse(\n",
    "            cocompared_values, (DLTFull_model(date_col, response_col, train_df, test_df))).astype(float)\n",
    "    except:\n",
    "        models_df.at[index , 'DLTFull'] = 100\n",
    "    try:\n",
    "        models_df.at[index , 'DLTAggregated'] = rmse(\n",
    "            cocompared_values, (DLTAggregated_model(date_col, response_col, train_df, test_df))).astype(float)\n",
    "    except:  \n",
    "        models_df.at[index , 'DLTAggregated'] = 100\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        models_df.at[index , 'LGTMAP'] = rmse(\n",
    "            cocompared_values, (LGTMAP_model(date_col, response_col, train_df, test_df))).astype(float)\n",
    "    except:\n",
    "        models_df.at[index , 'LGTMAP'] = 100\n",
    "    try:\n",
    "        models_df.at[index , 'LGTFull'] = rmse(\n",
    "            cocompared_values, (LGTFull_model(date_col, response_col, train_df, test_df))).astype(float)\n",
    "    except: \n",
    "        models_df.at[index , 'LGTFull'] = 100\n",
    "    try: \n",
    "        models_df.at[index , 'LGTAggregated'] = rmse(\n",
    "            cocompared_values, (LGTAggregated_model(date_col, response_col, train_df, test_df))).astype(float)\n",
    "    except:\n",
    "        models_df.at[index , 'LGTAggregated'] = 100\n",
    "\n",
    "    \n",
    "    # Using Pyro for Estimation\n",
    "    try:\n",
    "        models_df.at[index , 'LGTMAP_PyroEstimatorMAP'] = rmse(\n",
    "            cocompared_values, (LGTMAP_PyroEstimatorMAP(date_col, \n",
    "                                                    response_col, train_df, test_df))).astype(float)\n",
    "    except:\n",
    "        models_df.at[index , 'LGTMAP_PyroEstimatorMAP'] = 100\n",
    "    try:\n",
    "        models_df.at[index , 'LGTFull_pyro4'] = rmse(\n",
    "            cocompared_values, (LGTFull_pyro(date_col, response_col, train_df, test_df))).astype(float)\n",
    "    except:\n",
    "         models_df.at[index , 'LGTFull_pyro4'] = 100\n",
    "        \n",
    "    # Kernel-based Time-varying Regression (KTR)\n",
    "    try:\n",
    "        models_df.at[index , 'KTR_Lite_MAP'] = rmse(\n",
    "            cocompared_values, (ktrlite_MAP(date_col, response_col, train_df, test_df))).astype(float)\n",
    "    except:\n",
    "        models_df.at[index , 'KTR_Lite_MAP'] = 100\n",
    "    \n",
    "    \n",
    "    models_df.at[index, 'Curve Type'] = df[column].iloc[-1]\n",
    "        \n",
    "        \n",
    "    return models_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculating minimal RMSE value for each item"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def min_value(df):\n",
    "    \n",
    "    '''\n",
    "    Parameters:\n",
    "        df: input dataframe with multiple columns and values in a row\n",
    "    \n",
    "    Returns:\n",
    "        models_df: existing dataframe with added the new 'Model' column filled with \n",
    "        the name of the best-fitted model for each item\n",
    "    '''\n",
    "        \n",
    "    df.iloc[:, 1:-1].apply(pd.to_numeric)\n",
    "    df['Model'] = df.iloc[:, 1:-1].idxmin(axis=1)\n",
    "    \n",
    "    return models_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating Orbit models for each item"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\n",
    "\n",
    "\n",
    "tmp_df = pd.DataFrame()\n",
    "models_df = pd.DataFrame()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for index, column in enumerate(curve_df.columns[1:2]):\n",
    "    evaluating_models(index, column)\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "models_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "min_value(models_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \n",
    "\n",
    "-------------------------\n",
    "\n",
    "## ETS (which stands for Error, Trend, and Seasonality)\n",
    "\n",
    "### Methods of Estimations\n",
    "\n",
    "#### Maximum a Posteriori (MAP)\n",
    "\n",
    "> The advantage of MAP estimation is a **faster computational speed**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ETSMAP_model(date_col, response_col, train_df, test_df):\n",
    "    ets = ETSMAP(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "    )\n",
    "    \n",
    "    ets.fit(df=train_df)\n",
    "    predicted_df_MAP = ets.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_MAP['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full Bayesian Estimation\n",
    "\n",
    ">  Compared to MAP, it usually takes longer time to fit a full Bayesian models where **No-U-Turn Sampler (NUTS)** ([Hoffman and Gelman 2011](https://arxiv.org/abs/1111.4246)) is carried out under the hood. The advantage is that the inference and estimation are usually more robust."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ETSFull_model(date_col, response_col, train_df, test_df):\n",
    "    ets = ETSFull(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "        num_warmup=400,\n",
    "        num_sample=400,\n",
    "    )\n",
    "    \n",
    "    ets.fit(df=train_df)\n",
    "    predicted_df_ETSFull = ets.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_ETSFull['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aggregated Posteriors\n",
    "\n",
    "> Just like the full Bayesian method, it runs through the MCMC algorithm which is **NUTS** by default. The difference from a full model is that aggregated model first aggregates the posterior samples based on mean or median (via aggregate_method) then does the prediction using the aggreated posterior."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ETSAggregated_model(date_col, response_col, train_df, test_df):\n",
    "    ets = ETSAggregated(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "    )\n",
    "    ets.fit(df=train_df)\n",
    "    predicted_df_ETSAggregated = ets.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_ETSAggregated['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Damped Local Trend (DLT)\n",
    "\n",
    "> DLT is one of the main exponential smoothing models support in `orbit`. Performance is benchmarked with M3 monthly, M4 weekly dataset and some Uber internal dataset ([Ng and Wang et al., 2020](https://arxiv.org/abs/2004.08492)). The model is a fusion between the classical ETS ([Hyndman et. al., 2008](http://www.exponentialsmoothing.net/home))) with some refinement leveraging ideas from Rlgt ([Smyl et al., 2019](https://cran.r-project.org/web/packages/Rlgt/index.html)). \n",
    "\n",
    "### Global Trend Configurations\n",
    "\n",
    "#### Linear Global Trend"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# linear global trend\n",
    "def DLTMAP_lin(date_col, response_col, train_df, test_df):\n",
    "    dlt = DLTMAP(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "    )\n",
    "\n",
    "    dlt.fit(train_df)\n",
    "    predicted_df_DLTMAP_lin = dlt.predict(test_df)\n",
    "    \n",
    "    return predicted_df_DLTMAP_lin['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# log-linear global trend\n",
    "def DLTMAP_log_lin(date_col, response_col, train_df, test_df):\n",
    "    dlt = DLTMAP(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "        global_trend_option='loglinear'\n",
    "    )\n",
    "\n",
    "    dlt.fit(train_df)\n",
    "    predicted_df_DLTMAP_log_lin = dlt.predict(test_df)\n",
    "    \n",
    "    return predicted_df_DLTMAP_log_lin['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# log-linear global trend\n",
    "def DLTMAP_flat(date_col, response_col, train_df, test_df):\n",
    "    dlt = DLTMAP(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "        global_trend_option='flat'\n",
    "    )\n",
    "\n",
    "    dlt.fit(train_df)\n",
    "    predicted_df_DLTMAP_flat = dlt.predict(test_df)\n",
    "    \n",
    "    return predicted_df_DLTMAP_flat['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic Global Trend"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# logistic global trend\n",
    "def DLTMAP_logistic(date_col, response_col, train_df, test_df):\n",
    "    dlt = DLTMAP(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "        global_trend_option='logistic'\n",
    "    )\n",
    "\n",
    "    dlt.fit(train_df)\n",
    "    predicted_df_DLTMAP_logistic = dlt.predict(test_df)\n",
    "    \n",
    "    return predicted_df_DLTMAP_logistic['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Damped Local Trend Full Bayesian Estimation (DLTFull)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def DLTFull_model(date_col, response_col, train_df, test_df):\n",
    "    dlt = DLTFull(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        num_warmup=400,\n",
    "        num_sample=400,\n",
    "        seasonality=52,\n",
    "        seed=8888\n",
    "    )\n",
    "    \n",
    "    dlt.fit(df=train_df)\n",
    "    predicted_df_DLTFull = dlt.predict(df=test_df)\n",
    "\n",
    "    return predicted_df_DLTFull['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Damped Local Trend Full (DLTAggregated)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def DLTAggregated_model(date_col, response_col, train_df, test_df):\n",
    "    ets = DLTAggregated(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "    )\n",
    "    \n",
    "    ets.fit(df=train_df)\n",
    "    predicted_df_DLTAggregated = ets.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_DLTAggregated['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Local Global Trend (LGT) Model\n",
    "\n",
    "> LGT stands for Local and Global Trend and is a refined model from **Rlgt** ([Smyl et al., 2019](https://cran.r-project.org/web/packages/Rlgt/index.html)). The main difference is that LGT is an additive form taking log-transformation response as the modeling response. This essentially converts the model into a multicplicative with some advantages ([Ng and Wang et al., 2020](https://arxiv.org/abs/2004.08492)). \n",
    "\n",
    "### Local Global Trend Maximum a Posteriori (LGTMAP)\n",
    "\n",
    "> LGTMAP is the model class for MAP (Maximum a Posteriori) estimation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def LGTMAP_model(date_col, response_col, train_df, test_df):\n",
    "    lgt = LGTMAP(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "    )\n",
    "\n",
    "    lgt.fit(df=train_df)\n",
    "    predicted_df_LGTMAP = lgt.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_LGTMAP['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LGTFull\n",
    "\n",
    "> LGTFull is the model class for full Bayesian prediction. In full Bayesian prediction, the prediction will be conducted once for each parameter posterior sample, and the final prediction results are aggregated. Prediction will always return the median (aka 50th percentile) along with any additional percentiles that are provided."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def LGTFull_model(date_col, response_col, train_df, test_df):\n",
    "    lgt = LGTFull(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "    )\n",
    "\n",
    "    lgt.fit(df=train_df)\n",
    "    predicted_df_LGTFull = lgt.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_LGTFull['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Observations\n",
    "\n",
    "- **Time:** This model takes longer time to fit a full Bayesian models where No-U-Turn Sampler (NUTS) ([Hoffman and Gelman 2011](https://arxiv.org/abs/1111.4246)) is carried out under the hood."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LGTAggregated\n",
    "\n",
    "> LGTAggregated is the model class for aggregated posterior prediction. In aggregated prediction, the parameter posterior samples are reduced using `aggregate_method ({ 'mean', 'median' })` before performing a single prediction."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def LGTAggregated_model(date_col, response_col, train_df, test_df):\n",
    "    lgt = LGTAggregated(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "    )\n",
    "\n",
    "    lgt.fit(df=train_df)\n",
    "    predicted_df_LGTAggregated = lgt.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_LGTAggregated['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Pyro for Estimation\n",
    "\n",
    "> Currently are still experimenting Pyro and support Pyro only with LGT.\n",
    "\n",
    "### MAP Fit and Predict"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def LGTMAP_PyroEstimatorMAP(date_col, response_col, train_df, test_df):\n",
    "    lgt_map = LGTMAP(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "        estimator_type=PyroEstimatorMAP,\n",
    "    )\n",
    "\n",
    "    lgt_map.fit(df=train_df)\n",
    "    predicted_df_LGTMAP_pyro = lgt_map.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_LGTMAP_pyro['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### VI Fit and Predict"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def LGTFull_pyro(date_col, response_col, train_df, test_df):\n",
    "    lgt_vi = LGTFull(\n",
    "        response_col=response_col,\n",
    "        date_col=date_col,\n",
    "        seasonality=52,\n",
    "        seed=8888,\n",
    "        num_steps=101,\n",
    "        num_sample=100,\n",
    "        learning_rate=0.1,\n",
    "        n_bootstrap_draws=-1,\n",
    "        estimator_type=PyroEstimatorVI,\n",
    "    )\n",
    "\n",
    "    lgt_vi.fit(df=train_df)\n",
    "\n",
    "    predicted_df_LGTFull_pyro = lgt_vi.predict(df=test_df)\n",
    "    \n",
    "    return predicted_df_LGTFull_pyro['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kernel-based Time-varying Regression (KTR)\n",
    "\n",
    "> Implemented in the stable version of Orbit library\n",
    ">\n",
    "> [Documentation](https://orbit-ml.readthedocs.io/en/stable/tutorials/ktrlite.html)\n",
    "\n",
    "### KTRLite"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ktrlite_MAP(date_col, response_col, train_df, test_df):\n",
    "    ktrlite = KTRLiteMAP(\n",
    "        response_col=response_col,\n",
    "        #response_col=np.log(df[response_col]),\n",
    "        date_col=date_col,\n",
    "        level_knot_scale=.1,\n",
    "        span_level=.05,\n",
    "    )\n",
    "    \n",
    "    ktrlite.fit(train_df)\n",
    "    \n",
    "    predicted_df_ktrlite_MAP = ktrlite.predict(df=test_df, decompose=True)\n",
    "    \n",
    "    return predicted_df_ktrlite_MAP['prediction']"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}